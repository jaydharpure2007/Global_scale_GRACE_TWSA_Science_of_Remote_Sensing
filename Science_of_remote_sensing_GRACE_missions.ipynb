{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbda16f5",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da382dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "import pymannkendall as mk\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e00af",
   "metadata": {},
   "source": [
    "## Importance function used for model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc09ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_month_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate a list of monthly dates between start_date and end_date.\n",
    "    Returns dates in 'YYYY-MM' format.\n",
    "    \"\"\"\n",
    "    monthly_dates = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        monthly_dates.append(current_date.strftime('%Y-%m'))\n",
    "        current_date += relativedelta(months=1)\n",
    "    return monthly_dates\n",
    "\n",
    "def atoi(text):\n",
    "    \"\"\"\n",
    "    Convert text to an integer if it's a digit; otherwise, return the text.\n",
    "    \"\"\"\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    Sort strings in a natural way (e.g., file1, file2, ..., file10).\n",
    "    \"\"\"\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "def get_folder_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of all subdirectories in the given path.\n",
    "    \"\"\"\n",
    "    return [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "\n",
    "def split_x_y(sequences):\n",
    "    \"\"\"\n",
    "    Split input sequences into features (X) and targets (y).\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    X.append(sequences[:, :-1])\n",
    "    y.append(sequences[:, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def calculate_mean_value(tif_file):\n",
    "    \"\"\"\n",
    "    Calculate the mean value of a raster file, excluding nodata values.\n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_file) as src:\n",
    "        data = src.read(1, masked=True)\n",
    "        data_without_nodata = data[~data.mask]\n",
    "        mean_value = data_without_nodata.mean()\n",
    "    return mean_value\n",
    "\n",
    "def calculate_matrix(observed, predicted):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics between observed and predicted values.\n",
    "    \"\"\"\n",
    "    RMSE = np.sqrt(np.mean((predicted - observed)**2))\n",
    "    mean_observed = np.mean(observed)\n",
    "    NSE = 1 - (np.sum((predicted - observed)**2) / np.sum((observed - mean_observed)**2))\n",
    "    r = pearsonr(observed, predicted)\n",
    "    return round(RMSE, 3), round(NSE, 3), round(r[0], 3)\n",
    "\n",
    "def add_slope_to_filled_rasters(path_slope, imputed_path, output_folder):\n",
    "    \"\"\"\n",
    "    Adds the slope values to the filled raster files and saves the result to the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    path_slope (str): Directory path containing slope raster files.\n",
    "    imputed_path (str): Directory path containing the imputed (filled) raster files.\n",
    "    output_folder (str): Directory path where the output rasters will be saved.\n",
    "    \"\"\"\n",
    "    # List all .tif files in the slope directory\n",
    "    list_files = [f for f in os.listdir(path_slope) if f.endswith('.tif')]\n",
    "    list_files.sort(key=natural_keys)  # Sort files naturally\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Process each file\n",
    "    for f in list_files: \n",
    "        datasets = []\n",
    "\n",
    "        # Read the slope and filled raster files\n",
    "        slope_file = os.path.join(path_slope, f)\n",
    "        filled_file = os.path.join(imputed_path, f)\n",
    "\n",
    "        with rasterio.open(slope_file) as sl:\n",
    "            slp = sl.read(1)\n",
    "\n",
    "        datasets.append(slp)\n",
    "        with rasterio.open(filled_file) as fl:\n",
    "            flp = fl.read(1)\n",
    "        datasets.append(flp)\n",
    "\n",
    "        # Create a mask for nodata values in the filled raster\n",
    "        nodata_mask = (flp == fl.nodata)\n",
    "\n",
    "        # Add the slope to the filled raster, excluding nodata values\n",
    "        result = np.where(nodata_mask, flp, datasets[0] + datasets[1])\n",
    "\n",
    "        # Update metadata with nodata information\n",
    "        meta = fl.meta.copy()\n",
    "        meta.update(nodata=fl.nodata)\n",
    "\n",
    "        # Create a new GeoTIFF file to store the result\n",
    "        output_file = os.path.join(output_folder, f)\n",
    "        with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "            dst.write(result.astype(meta['dtype']), 1)\n",
    "\n",
    "        print(f\"Processed: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642ff18",
   "metadata": {},
   "source": [
    "## Automated Raster-based Trend and detrended Analysis of Hydroclimatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio import mask\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from stldecompose import STL\n",
    "import pymannkendall as mk\n",
    "import re\n",
    "\n",
    "\n",
    "def data_retrieve_withna(fol_files, date_range, array):\n",
    "    \"\"\"\n",
    "    Retrieve data with missing values for the given date range.\n",
    "    Missing entries are replaced with NaN.\n",
    "    \"\"\"\n",
    "    data_with_missing = []\n",
    "    i = 0\n",
    "    for file in date_range:\n",
    "        if file in fol_files:\n",
    "            data_with_missing.append(array[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            data_with_missing.append(np.nan)\n",
    "    return data_with_missing\n",
    "\n",
    "\n",
    "def stl_method(data_without_na, array_miss):\n",
    "    \"\"\"\n",
    "    Apply STL decomposition to extract trend and seasonal components.\n",
    "    Compute the linear trend using Mann-Kendall trend analysis.\n",
    "    \"\"\"\n",
    "    stl_result = STL(data_without_na, period=12, seasonal=13, trend_deg=0, seasonal_deg=0, low_pass_deg=0, robust=True).fit()\n",
    "    trend_component = stl_result.trend\n",
    "\n",
    "    # Perform Mann-Kendall trend analysis\n",
    "    mk_result = mk.original_test(trend_component)\n",
    "    linear_trend = [(x + 1) * mk_result.slope + mk_result.intercept for x in range(len(array_miss))]\n",
    "\n",
    "    # Calculate Detrended\n",
    "    detrended = np.array(array_miss) - np.array(linear_trend)\n",
    "    return linear_trend, detrended\n",
    "\n",
    "\n",
    "# Define the path to the base directory\n",
    "path = r\"C:\\Users\\YourUsername\\model\"\n",
    "\n",
    "# Get the list of subdirectories\n",
    "folder_list = get_folder_list(path)\n",
    "\n",
    "for folder in folder_list:\n",
    "    # Define the path for the current folder\n",
    "    fol_path = os.path.join(path, folder)\n",
    "    print(f\"Processing folder: {fol_path}\")\n",
    "\n",
    "    # Get the list of .tif files and sort them naturally\n",
    "    fol_files = [f for f in os.listdir(fol_path) if f.endswith('.tif')]\n",
    "    fol_files.sort(key=natural_keys)\n",
    "\n",
    "    # Read reference raster metadata\n",
    "    with rasterio.open(os.path.join(fol_path, fol_files[0])) as ref:\n",
    "        ref_data = ref.read(1, masked=True)\n",
    "        profile = ref.profile\n",
    "        nodata = ref.nodata\n",
    "\n",
    "    # Define date range for the analysis\n",
    "    start_date = datetime(2002, 4, 1)\n",
    "    end_date = datetime(2022, 8, 31)\n",
    "    date_range = [f + \".tif\" for f in create_month_range(start_date, end_date)]\n",
    "\n",
    "    # Read all raster files into an array\n",
    "    raster_array = []\n",
    "    for filename in fol_files:\n",
    "        with rasterio.open(os.path.join(fol_path, filename)) as src:\n",
    "            input_data = src.read(1, masked=True)\n",
    "            raster_array.append(input_data)\n",
    "\n",
    "    raster_array = np.array(raster_array)\n",
    "    n_sample, row, col = raster_array.shape\n",
    "\n",
    "    # Create output directories for slope and detrended files\n",
    "    slope_path = os.path.join(fol_path, \"Slope\")\n",
    "    os.makedirs(slope_path, exist_ok=True)\n",
    "\n",
    "    detrended_path = os.path.join(fol_path, \"Detrended\")\n",
    "    os.makedirs(detrended_path, exist_ok=True)\n",
    "\n",
    "    # Process each pixel in the raster data\n",
    "    lt = []  # Linear trend\n",
    "    dt = []  # Detrended\n",
    "\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            if raster_array[0, r, c] != nodata:\n",
    "                # Retrieve data for the current pixel\n",
    "                array_miss = data_retrieve_withna(fol_files, date_range, raster_array[:, r, c])\n",
    "                data_without_na = [x for x in array_miss if not np.isnan(x)]\n",
    "\n",
    "                # Perform STL decomposition and compute detrended\n",
    "                linear_trend, detrended = stl_method(data_without_na, array_miss)\n",
    "                lt.append(linear_trend)\n",
    "                dt.append(detrended)\n",
    "            else:\n",
    "                # Fill missing values with nodata\n",
    "                array_miss = data_retrieve_withna(fol_files, date_range, raster_array[:, r, c])\n",
    "                lt.append([nodata] * len(array_miss))\n",
    "                dt.append(array_miss)\n",
    "\n",
    "    # Reshape results back into raster dimensions\n",
    "    lt = np.array(lt).reshape(row, col, -1)\n",
    "    dt = np.array(dt).reshape(row, col, -1)\n",
    "\n",
    "    # Save the linear trend and detrended as raster files\n",
    "    for sam in range(lt.shape[-1]):\n",
    "        with rasterio.open(os.path.join(slope_path, date_range[sam]), \"w\", **profile) as dst:\n",
    "            dst.write(lt[:, :, sam], 1)\n",
    "        with rasterio.open(os.path.join(detrended_path, date_range[sam]), \"w\", **profile) as dst:\n",
    "            dst.write(dt[:, :, sam], 1)\n",
    "\n",
    "        print(f\"File {sam + 1}/{lt.shape[-1]} saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cd7df",
   "metadata": {},
   "source": [
    "## Preparation of training, testing, and gap-filling datasets for a model using raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration and initialization\n",
    "# Path to the detrended model data folder\n",
    "path = r'C:\\Users\\YourUsername\\model\\Detrended'\n",
    "vars_list = get_folder_list(path)  # Get list of all variable folders\n",
    "\n",
    "# Define the target variable and remove it from the variable list\n",
    "target_var = 'GRACE TWSA'  # Define the exact name of the folder of target\n",
    "vars_list.remove(target_var)  # Exclude GRACE TWSA from the list of input variables\n",
    "input_vars = vars_list  # Remaining variables as input variables\n",
    "\n",
    "# List and sort all target files (TIF files)\n",
    "target_files = [f for f in os.listdir(os.path.join(path, target_var)) if f.endswith('.tif')]\n",
    "target_files.sort(key=natural_keys)\n",
    "\n",
    "# Get files for the first input variable\n",
    "var1_files = [f for f in os.listdir(os.path.join(path, input_vars[0])) if f.endswith('.tif')]\n",
    "var1_files.sort(key=natural_keys)\n",
    "\n",
    "# Define specific files for training and testing\n",
    "Data_to_find = [target_files[0], '2017-06.tif', '2018-06.tif', target_files[-1]]\n",
    "\n",
    "# Find indices for the selected files\n",
    "indices = [index for index, value in enumerate(target_files) if value in Data_to_find]\n",
    "\n",
    "# Separate training and testing data\n",
    "train_data = target_files[indices[0]:indices[1] + 1]\n",
    "test_data = target_files[indices[2]:indices[3] + 1]\n",
    "\n",
    "# Gap-filling data (first and last file for each variable)\n",
    "Data_gap_find = [var1_files[0], var1_files[-1]]\n",
    "indices_fill = [index for index, value in enumerate(var1_files) if value in Data_gap_find]\n",
    "gap_data = var1_files[indices_fill[0]:indices_fill[-1] + 1]\n",
    "\n",
    "# Read metadata from the first target raster\n",
    "with rasterio.open(os.path.join(path, target_var, target_files[0])) as ref:\n",
    "    ref_data = ref.read(1, masked=True)\n",
    "    profile = ref.profile\n",
    "\n",
    "### Data preparation\n",
    "# Prepare training data as a dictionary of stacked arrays\n",
    "train_array = {}\n",
    "for index, filename in enumerate(train_data):\n",
    "    raster_array = []\n",
    "    for var in input_vars:  # Iterate over input variables\n",
    "        try:\n",
    "            with rasterio.open(os.path.join(path, var, filename)) as src:\n",
    "                input_data = src.read(1, masked=True)  # Read raster data\n",
    "                raster_array.append(input_data)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename} in variable {var}\")\n",
    "    try:\n",
    "        with rasterio.open(os.path.join(path, target_var, filename)) as src_t:\n",
    "            target_data = src_t.read(1, masked=True)  # Read target data\n",
    "            raster_array.append(target_data)\n",
    "        train_array[index] = np.stack(raster_array, axis=0)  # Stack arrays along a new dimension\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename} in target variable\")\n",
    "\n",
    "# Prepare testing data (similar to training data)\n",
    "test_array = {}\n",
    "for index, filename in enumerate(test_data):\n",
    "    raster_array = []\n",
    "    for var in input_vars:\n",
    "        try:\n",
    "            with rasterio.open(os.path.join(path, var, filename)) as src:\n",
    "                input_data = src.read(1, masked=True)\n",
    "                raster_array.append(input_data)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename} in variable {var}\")\n",
    "    try:\n",
    "        with rasterio.open(os.path.join(path, target_var, filename)) as src_t:\n",
    "            target_data = src_t.read(1, masked=True)\n",
    "            raster_array.append(target_data)\n",
    "        test_array[index] = np.stack(raster_array, axis=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename} in target variable\")\n",
    "\n",
    "# Prepare gap-filling data (similar to training and testing)\n",
    "gap_array = {}\n",
    "for index, filename in enumerate(gap_data):\n",
    "    raster_array = []\n",
    "    for var in input_vars:\n",
    "        try:\n",
    "            with rasterio.open(os.path.join(path, var, filename)) as src:\n",
    "                input_data = src.read(1, masked=True)\n",
    "                raster_array.append(input_data)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename} in variable {var}\")\n",
    "    try:\n",
    "        gap_array[index] = np.stack(raster_array, axis=0)\n",
    "    except ValueError:\n",
    "        print(f\"Error stacking array for gap-filling data at index {index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9975f0d",
   "metadata": {},
   "source": [
    "## Data standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store mean and standard deviation values for gap samples\n",
    "gap_mean_array = []\n",
    "gap_std_array = []\n",
    "\n",
    "# Loop through each feature\n",
    "for feature in range(n_features):\n",
    "    # Handle all features except the last one\n",
    "    if feature != n_features - 1:\n",
    "        # Iterate over each row and column in the data\n",
    "        for r in range(row):\n",
    "            for c in range(col):\n",
    "                \n",
    "                gap_temp = []\n",
    "\n",
    "                # Normalize data for training samples\n",
    "                for train_sample in train_samples:\n",
    "                    train_temp.append(train_array[train_sample][feature, r, c])\n",
    "\n",
    "                # Check if the first value is not a missing value (e.g., 9999.0)\n",
    "                if np.array(train_temp)[0] != 9999.0:\n",
    "                    train_mean = np.array(train_temp).mean()\n",
    "                    train_std = np.array(train_temp).std()\n",
    "\n",
    "                    # Check if standard deviation is non-zero for normalization\n",
    "                    if train_std != 0.0:\n",
    "                        train_normalized = (train_temp - train_mean) / train_std\n",
    "                        train_norm[feature, r, c] = train_normalized\n",
    "                    else:  # If std is zero, only subtract the mean\n",
    "                        train_normalized = (train_temp - train_mean)\n",
    "                        train_norm[feature, r, c] = train_normalized\n",
    "                else:\n",
    "                    # If missing value, retain original values\n",
    "                    train_norm[feature, r, c] = np.array(train_temp)\n",
    "\n",
    "                # Normalize data for testing samples\n",
    "                for test_sample in test_samples:\n",
    "                    test_temp.append(test_array[test_sample][feature, r, c])\n",
    "\n",
    "                if np.array(test_temp)[0] != 9999.0:\n",
    "                    test_mean = np.array(test_temp).mean()\n",
    "                    test_std = np.array(test_temp).std()\n",
    "\n",
    "                    if test_std != 0.0:\n",
    "                        test_normalized = (test_temp - test_mean) / test_std\n",
    "                        test_norm[feature, r, c] = test_normalized\n",
    "                    else:\n",
    "                        test_normalized = (test_temp - test_mean)\n",
    "                        test_norm[feature, r, c] = test_normalized\n",
    "                else:\n",
    "                    test_norm[feature, r, c] = np.array(test_temp)\n",
    "\n",
    "                # Normalize data for gap samples\n",
    "                for gap_sample in gap_samples:\n",
    "                    gap_temp.append(gap_array[gap_sample][feature, r, c])\n",
    "\n",
    "                if np.array(gap_temp)[0] != 9999.0:\n",
    "                    gap_mean = np.array(gap_temp).mean()\n",
    "                    gap_std = np.array(gap_temp).std()\n",
    "\n",
    "                    if gap_std != 0.0:\n",
    "                        gap_normalized = (gap_temp - gap_mean) / gap_std\n",
    "                        gap_norm[feature, r, c] = gap_normalized\n",
    "                    else:\n",
    "                        gap_normalized = (gap_temp - gap_mean)\n",
    "                        gap_norm[feature, r, c] = gap_normalized\n",
    "                else:\n",
    "                    gap_norm[feature, r, c] = np.array(gap_temp)\n",
    "\n",
    "    else:  # Special handling for the last feature\n",
    "        for r in range(row):\n",
    "            for c in range(col):\n",
    "                train_temp = []\n",
    "                test_temp = []\n",
    "                gap_temp = []\n",
    "\n",
    "                # Normalize data for training samples\n",
    "                for train_sample in train_samples:\n",
    "                    train_temp.append(train_array[train_sample][feature, r, c])\n",
    "\n",
    "                if np.array(train_temp)[0] != -99999.0:\n",
    "                    train_mean = np.array(train_temp).mean()\n",
    "                    train_std = np.array(train_temp).std()\n",
    "\n",
    "                    if train_std != 0.0:\n",
    "                        train_normalized = (train_temp - train_mean) / train_std\n",
    "                        train_norm[feature, r, c] = train_normalized\n",
    "                    else:\n",
    "                        train_normalized = (train_temp - train_mean)\n",
    "                        train_norm[feature, r, c] = train_normalized\n",
    "                else:\n",
    "                    train_norm[feature, r, c] = np.array(train_temp)\n",
    "\n",
    "                # Normalize data for testing samples\n",
    "                for test_sample in test_samples:\n",
    "                    test_temp.append(test_array[test_sample][feature, r, c])\n",
    "\n",
    "                if np.array(test_temp)[0] != -99999.0:\n",
    "                    test_mean = np.array(test_temp).mean()\n",
    "                    test_std = np.array(test_temp).std()\n",
    "\n",
    "                    if test_std != 0.0:\n",
    "                        test_normalized = (test_temp - test_mean) / test_std\n",
    "                        test_norm[feature, r, c] = test_normalized\n",
    "                    else:\n",
    "                        test_normalized = (test_temp - test_mean)\n",
    "                        test_norm[feature, r, c] = test_normalized\n",
    "                else:\n",
    "                    test_norm[feature, r, c] = np.array(test_temp)\n",
    "\n",
    "# Reorganize and reshape normalized arrays for gap data\n",
    "gap_norm_image = []\n",
    "gap_mean_array = []\n",
    "gap_std_array = []\n",
    "\n",
    "for k in gap_norm.keys():\n",
    "    gap_norm_image.append(gap_norm[k])\n",
    "gap_norm_image = np.array(gap_norm_image).reshape(n_features - 1, row, col, np.array(gap_norm_image).shape[1])\n",
    "\n",
    "for index in range(row * col):\n",
    "    gap_mean_array.append((train_mean_array[index] + test_mean_array[index]) / 2)\n",
    "    gap_std_array.append((train_std_array[index] + test_std_array[index]) / 2)\n",
    "gap_mean_array = np.array(gap_mean_array).reshape(row, col)\n",
    "gap_std_array = np.array(gap_std_array).reshape(row, col)\n",
    "\n",
    "# Reorganize normalized training data\n",
    "train_norm_image = []\n",
    "for k in train_norm.keys():\n",
    "    train_norm_image.append(train_norm[k])\n",
    "train_norm_image = np.array(train_norm_image).reshape(n_features, row, col, np.array(train_norm_image).shape[1])\n",
    "\n",
    "# Reorganize normalized testing data\n",
    "test_norm_image = []\n",
    "for k in test_norm.keys():\n",
    "    test_norm_image.append(test_norm[k])\n",
    "test_norm_image = np.array(test_norm_image).reshape(n_features, row, col, np.array(test_norm_image).shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd858e90",
   "metadata": {},
   "source": [
    "## ANN model applied on Detrended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Model Start\n",
    "print(\"*** ANN model start processing***\")\n",
    "\n",
    "# Define ANN Model Class\n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.bn1 = nn.LayerNorm(32)  # Better for small batch sizes\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc3 = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.bn1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define no data value\n",
    "nodata = -99999.0\n",
    "\n",
    "# Initialize output storage\n",
    "whole_data_fill = []\n",
    "\n",
    "# Check if CUDA is available for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Main Training and Prediction Loop\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        if train_norm_image[-1, r, c, 0] != nodata:\n",
    "            x_train_in, y_train_in = split_x_y(train_norm_image[:, r, c, :].T)\n",
    "            x_test_in, y_test_in = split_x_y(test_norm_image[:, r, c, :].T)\n",
    "            x_gap_in = gap_norm_image[:, r, c, :].T\n",
    "\n",
    "            # Convert to tensors and move to device\n",
    "            x_train_in = torch.tensor(x_train_in).float().to(device)\n",
    "            y_train_in = torch.tensor(y_train_in.T).float().to(device)\n",
    "            x_test_in = torch.tensor(x_test_in).float().to(device)\n",
    "            y_test_in = torch.tensor(y_test_in.T).float().to(device)\n",
    "            x_gap_in = torch.tensor(x_gap_in).float().to(device)\n",
    "\n",
    "            # Prepare datasets and dataloaders\n",
    "            train_dataset = TensorDataset(x_train_in, y_train_in)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "            test_dataset = TensorDataset(x_test_in, y_test_in)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "            # Initialize model\n",
    "            input_size = x_train_in.shape[1]\n",
    "            model = FullyConnectedLayer(input_size, output_size, dropout_prob).to(device)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Include L2 regularization via weight_decay\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0035, weight_decay=0.01)  # L2 reg applied here\n",
    "\n",
    "            # Training Loop\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 10\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(1000):\n",
    "                model.train()\n",
    "                for inputs, targets in train_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Validation Loss\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, targets in test_dataloader:\n",
    "                        outputs = model(inputs)\n",
    "                        val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        break\n",
    "\n",
    "            # Prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gap_preds = model(x_gap_in).cpu().numpy()\n",
    "                gap_preds = gap_preds * gap_std_array[r, c] + gap_mean_array[r, c]\n",
    "                whole_data_fill[r, c, :] = gap_preds\n",
    "\n",
    "# Save the filled data\n",
    "n_sample = np.array(whole_data_fill).shape[-1]\n",
    "imputed_path = r\"C:\\Users\\YourUsername\\model\\Results\\ANN\"\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "whole_data_fill = np.array(whole_data_fill).reshape(row, col, n_sample)\n",
    "for sample in range(n_sample):\n",
    "    temp = whole_data_fill[:, :, sample].astype(np.float32)\n",
    "    with rasterio.open(imputed_path + var1_files[sample], \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "\n",
    "# Add slope to the filled rasters\n",
    "path_slope = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\\Slope\"\n",
    "output_folder = os.path.join(imputed_path, \"Added Slope\")\n",
    "add_slope_to_filled_rasters(path_slope, imputed_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc807f",
   "metadata": {},
   "source": [
    "## SLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Start\n",
    "print(\"*** SLSTM model start processing***\")\n",
    "\n",
    "# Function to convert time series to supervised learning format\n",
    "def series_to_supervised(sequences, n_steps_in=1, n_steps_out=1):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]  # Input and output split\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to convert sequences to supervised data (gap data)\n",
    "def series_to_supervised_gap(sequences, n_steps_in=1, n_steps_out=1):\n",
    "    X = list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        seq_x = sequences[i:end_ix]\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "# Define no data value\n",
    "nodata = -99999.0\n",
    "\n",
    "# Initialize output storage\n",
    "whole_data_fill = []\n",
    "\n",
    "# Check if CUDA is available for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "time_step = 3  # Time step for sequence length\n",
    "\n",
    "# Loop through each grid cell\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        if train_norm_image[-1, r, c, 0].T != nodata:  # Check for valid data\n",
    "            print(\"Processing grid cell:\", r, c)\n",
    "\n",
    "            # Prepare input and output sequences for training and testing\n",
    "            x_train_in, y_train_in = series_to_supervised(train_norm_image[:, r, c, :].T, n_steps_in=time_step, n_steps_out=1)\n",
    "            x_test_in, y_test_in = series_to_supervised(test_norm_image[:, r, c, :].T, n_steps_in=time_step, n_steps_out=1)\n",
    "            x_gap_in = series_to_supervised_gap(gap_norm_image[:, r, c, :].T, n_steps_in=time_step)\n",
    "\n",
    "            # Convert to PyTorch tensors\n",
    "            x_train_in, y_train_in = torch.tensor(x_train_in).float(), torch.tensor(y_train_in.T).float()\n",
    "            x_test_in, y_test_in = torch.tensor(x_test_in).float(), torch.tensor(y_test_in.T).float()\n",
    "            x_gap_in = torch.tensor(x_gap_in).float()\n",
    "\n",
    "            # Move tensors to device\n",
    "            x_train_in, y_train_in = x_train_in.to(device), y_train_in.to(device)\n",
    "            x_test_in, y_test_in = x_test_in.to(device), y_test_in.to(device)\n",
    "            x_gap_in = x_gap_in.to(device)\n",
    "\n",
    "            # Define LSTM model class\n",
    "            class LSTM(nn.Module):\n",
    "                def __init__(self, input_size, hidden_sizes, output_size, dropout_rates, weight_decay=0.01):\n",
    "                    super(LSTM, self).__init__()\n",
    "                    self.lstm_layers = nn.ModuleList([\n",
    "                        nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n",
    "                                hidden_size,\n",
    "                                num_layers=2,\n",
    "                                dropout=dropout_rates[i],\n",
    "                                batch_first=True)\n",
    "                        for i, hidden_size in enumerate(hidden_sizes)\n",
    "                    ])\n",
    "                    self.linear = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    for lstm_layer in self.lstm_layers:\n",
    "                        x, _ = lstm_layer(x)\n",
    "                    out = self.linear(x[:, -1, :])\n",
    "                    return out\n",
    "\n",
    "            # Define model hyperparameters\n",
    "            input_size = x_train_in.shape[-1]\n",
    "            output_size = 1\n",
    "            hidden_sizes = [150, 100]\n",
    "            dropout_rates = [0.1, 0.1]\n",
    "            weight_decay = 0.0001\n",
    "            batch_size = 8\n",
    "\n",
    "            # Create DataLoader\n",
    "            train_dataset = TensorDataset(x_train_in, y_train_in)\n",
    "            test_dataset = TensorDataset(x_test_in, y_test_in)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize model\n",
    "            model = LSTM(input_size, hidden_sizes, output_size, dropout_rates, weight_decay).to(device)\n",
    "\n",
    "            # Define loss and optimizer\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0035, weight_decay=weight_decay)\n",
    "\n",
    "            # Training loop\n",
    "            best_val_loss = float('inf')\n",
    "            patience = 10\n",
    "            counter = 0\n",
    "\n",
    "            for epoch in range(1000):\n",
    "                model.train()\n",
    "                for inputs, targets in train_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Validation loop\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, targets in test_dataloader:\n",
    "                        outputs = model(inputs)\n",
    "                        val_loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "                        test_loss += val_loss.item()\n",
    "                avg_test_loss = test_loss / len(test_dataloader)\n",
    "\n",
    "                # Early stopping\n",
    "                if avg_test_loss < best_val_loss:\n",
    "                    best_val_loss = avg_test_loss\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        break\n",
    "\n",
    "            # Predict gap data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gap_preds = model(x_gap_in).cpu().numpy()\n",
    "                gap_preds = gap_preds * gap_std_array[r, c] + gap_mean_array[r, c]\n",
    "                whole_data_fill.append(gap_preds.ravel())\n",
    "        else:\n",
    "            f_nodata = [nodata for _ in range(gap_norm_image[-1, 0, 0, :].T.shape[0] - time_step + 1)]\n",
    "            whole_data_fill.append(f_nodata)\n",
    "\n",
    "# Reshape and save output\n",
    "whole_data_fill = np.array(whole_data_fill).reshape(row, col, -1)\n",
    "\n",
    "imputed_path = r\"C:\\Users\\YourUsername\\model\\Results\\LSTM\"\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "n_sample = whole_data_fill.shape[-1]\n",
    "for sample in range(n_sample):\n",
    "    temp = whole_data_fill[:, :, sample].astype(np.float32)\n",
    "    with rasterio.open(os.path.join(imputed_path, var1_files[sample + time_step - 1]), \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "        print(f\"Saved: {var1_files[sample + time_step - 1]}\")\n",
    "\n",
    "# Add slope\n",
    "path_slope = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\\Slope\"\n",
    "output_folder = os.path.join(imputed_path, \"Added Slope\")\n",
    "add_slope_to_filled_rasters(path_slope, imputed_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863097e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print process initiation\n",
    "print(\"## Random Forest Processing start... ##\")\n",
    "\n",
    "# Define NoData value and initialize storage for filled data\n",
    "nodata = -99999.0\n",
    "whole_data_fill = []\n",
    "\n",
    "# Loop through rows and columns of the data\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        # Check if the pixel value is not NoData\n",
    "        if train_norm_image[-1, r, c, 0].T != nodata:\n",
    "            # Split normalized training data into input and output\n",
    "            x_train_in, y_train_in = split_x_y(train_norm_image[:, r, c, :].T)\n",
    "            x_train_in, y_train_in = x_train_in.squeeze(), y_train_in.squeeze()\n",
    "\n",
    "            # Split normalized testing data into input and output\n",
    "            x_test_in, y_test_in = split_x_y(test_norm_image[:, r, c, :].T)\n",
    "            x_test_in, y_test_in = x_test_in.squeeze(), y_test_in.squeeze()\n",
    "\n",
    "            # Prepare the normalized gap data for prediction\n",
    "            x_gap_in = gap_norm_image[:, r, c, :].T\n",
    "            x_gap_in = x_gap_in.squeeze()\n",
    "\n",
    "            # Train a Random Forest regressor\n",
    "            rf_regressor = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "            rf_regressor.fit(x_train_in, y_train_in.ravel())\n",
    "\n",
    "            # Predict gap values\n",
    "            gap_preds = rf_regressor.predict(x_gap_in)\n",
    "\n",
    "            # Denormalize predicted values using the stored mean and standard deviation\n",
    "            gap_preds = gap_preds * gap_std_array[r, c] + gap_mean_array[r, c]\n",
    "\n",
    "            # Append the predictions for the current pixel to the storage\n",
    "            whole_data_fill.append(np.array(gap_preds).ravel())\n",
    "        else:\n",
    "            # Handle NoData pixels by filling with NoData value\n",
    "            f_nodata = [nodata for _ in range(gap_norm_image[-1, 0, 0, :].T.shape[0])]\n",
    "            whole_data_fill.append(np.array(f_nodata))\n",
    "\n",
    "# Reshape the filled data to the original dimensions\n",
    "n_sample = np.array(whole_data_fill).shape[-1]\n",
    "whole_data_fill = np.array(whole_data_fill).reshape(row, col, n_sample)\n",
    "\n",
    "# Define output path for the imputed data and create directory if it doesn't exist\n",
    "imputed_path = r\"C:\\Users\\YourUsername\\model\\Results\\RF\"\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "# Save the filled data as raster files\n",
    "for sample in range(n_sample):\n",
    "    temp = whole_data_fill[:, :, sample].astype(np.float32)\n",
    "    with rasterio.open(imputed_path + var1_files[sample], \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "        print(var1_files[sample])\n",
    "        \n",
    "# Add slope\n",
    "path_slope = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\\Slope\"\n",
    "output_folder = os.path.join(imputed_path, \"Added Slope\")\n",
    "add_slope_to_filled_rasters(path_slope, imputed_path, output_folder)\n",
    "print(\"## Random Forest Processing completed successfully. ##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69347a28",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e148f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print process initiation\n",
    "print(\"## Support Vector Processing start... ##\")\n",
    "\n",
    "# Define NoData value and initialize storage for filled data\n",
    "nodata = -99999.0\n",
    "whole_data_fill = []\n",
    "\n",
    "# Loop through rows and columns of the data\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        # Check if the pixel value is not NoData\n",
    "        if train_norm_image[-1, r, c, 0].T != nodata:\n",
    "            # Split normalized training data into input and output\n",
    "            x_train_in, y_train_in = split_x_y(train_norm_image[:, r, c, :].T)\n",
    "            x_train_in, y_train_in = x_train_in.squeeze(), y_train_in.squeeze()\n",
    "\n",
    "            # Split normalized testing data into input and output\n",
    "            x_test_in, y_test_in = split_x_y(test_norm_image[:, r, c, :].T)\n",
    "            x_test_in, y_test_in = x_test_in.squeeze(), y_test_in.squeeze()\n",
    "\n",
    "            # Prepare the normalized gap data for prediction\n",
    "            x_gap_in = gap_norm_image[:, r, c, :].T\n",
    "            x_gap_in = x_gap_in.squeeze()\n",
    "\n",
    "            # Train a Support Vector regressor\n",
    "            svr_regressor = SVR(kernel='rbf')\n",
    "            svr_regressor.fit(x_train_in, y_train_in.ravel())\n",
    "\n",
    "            # Predict gap values\n",
    "            gap_preds = svr_regressor.predict(x_gap_in)\n",
    "\n",
    "            # Denormalize predicted values using the stored mean and standard deviation\n",
    "            gap_preds = gap_preds * gap_std_array[r, c] + gap_mean_array[r, c]\n",
    "\n",
    "            # Append the predictions for the current pixel to the storage\n",
    "            whole_data_fill.append(np.array(gap_preds).ravel())\n",
    "        else:\n",
    "            # Handle NoData pixels by filling with NoData value\n",
    "            f_nodata = [nodata for _ in range(gap_norm_image[-1, 0, 0, :].T.shape[0])]\n",
    "            whole_data_fill.append(np.array(f_nodata))\n",
    "\n",
    "# Reshape the filled data to the original dimensions\n",
    "n_sample = np.array(whole_data_fill).shape[-1]\n",
    "whole_data_fill = np.array(whole_data_fill).reshape(row, col, n_sample)\n",
    "\n",
    "# Define output path for the imputed data and create directory if it doesn't exist\n",
    "imputed_path = r\"C:\\Users\\YourUsername\\model\\Results\\SVR\"\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "# Save the filled data as raster files\n",
    "for sample in range(n_sample):\n",
    "    temp = whole_data_fill[:, :, sample].astype(np.float32)\n",
    "    with rasterio.open(imputed_path + var1_files[sample], \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "        print(var1_files[sample])\n",
    "\n",
    "# Add slope\n",
    "path_slope = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\\Slope\"\n",
    "output_folder = os.path.join(imputed_path, \"Added Slope\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Assuming add_slope_to_filled_rasters is defined elsewhere\n",
    "add_slope_to_filled_rasters(path_slope, imputed_path, output_folder)\n",
    "\n",
    "print(\"## Support Vector Processing completed successfully. ##\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9df0e",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3203504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print process initiation\n",
    "print(\"## XGBRegressor Processing start... ##\")\n",
    "\n",
    "# Define NoData value and initialize storage for filled data\n",
    "nodata = -99999.0\n",
    "whole_data_fill = []\n",
    "\n",
    "# Loop through rows and columns of the data\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        # Check if the pixel value is not NoData\n",
    "        if train_norm_image[-1, r, c, 0].T != nodata:\n",
    "            # Split normalized training data into input and output\n",
    "            x_train_in, y_train_in = split_x_y(train_norm_image[:, r, c, :].T)\n",
    "            x_train_in, y_train_in = x_train_in.squeeze(), y_train_in.squeeze()\n",
    "\n",
    "            # Split normalized testing data into input and output\n",
    "            x_test_in, y_test_in = split_x_y(test_norm_image[:, r, c, :].T)\n",
    "            x_test_in, y_test_in = x_test_in.squeeze(), y_test_in.squeeze()\n",
    "\n",
    "            # Prepare the normalized gap data for prediction\n",
    "            x_gap_in = gap_norm_image[:, r, c, :].T\n",
    "            x_gap_in = x_gap_in.squeeze()\n",
    "            \n",
    "            # Create an XGBoost Regressor\n",
    "            # Initialize the XGBRegressor with specified parameters\n",
    "            xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,  # Sets the learning rate to 0.1\n",
    "                max_depth=6,        # Sets the maximum depth of a tree to 6\n",
    "                n_estimators=100    # Sets the number of trees to 100\n",
    "            )\n",
    "            xgb_regressor.fit(x_train_in, y_train_in.ravel())\n",
    "\n",
    "            # Predict gap values\n",
    "            gap_preds = xgb_regressor.predict(x_gap_in)\n",
    "\n",
    "            # Denormalize predicted values using the stored mean and standard deviation\n",
    "            gap_preds = gap_preds * gap_std_array[r, c] + gap_mean_array[r, c]\n",
    "\n",
    "            # Append the predictions for the current pixel to the storage\n",
    "            whole_data_fill.append(np.array(gap_preds).ravel())\n",
    "        else:\n",
    "            # Handle NoData pixels by filling with NoData value\n",
    "            f_nodata = [nodata for _ in range(gap_norm_image[-1, 0, 0, :].T.shape[0])]\n",
    "            whole_data_fill.append(np.array(f_nodata))\n",
    "\n",
    "# Reshape the filled data to the original dimensions\n",
    "n_sample = np.array(whole_data_fill).shape[-1]\n",
    "whole_data_fill = np.array(whole_data_fill).reshape(row, col, n_sample)\n",
    "\n",
    "# Define output path for the imputed data and create directory if it doesn't exist\n",
    "imputed_path = r\"C:\\Users\\YourUsername\\model\\Results\\XGB\"\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "# Save the filled data as raster files\n",
    "for sample in range(n_sample):\n",
    "    temp = whole_data_fill[:, :, sample].astype(np.float32)\n",
    "    with rasterio.open(os.path.join(imputed_path, var1_files[sample]), \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "        print(var1_files[sample])\n",
    "\n",
    "# Add slope\n",
    "path_slope = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\\Slope\"\n",
    "output_folder = os.path.join(imputed_path, \"Added Slope\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Assuming add_slope_to_filled_rasters is defined elsewhere\n",
    "add_slope_to_filled_rasters(path_slope, imputed_path, output_folder)\n",
    "\n",
    "print(\"## XGBRegressor Processing completed successfully. ##\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df4fe",
   "metadata": {},
   "source": [
    "## Calculate performance metrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "model_name = \"ANN\"\n",
    "\n",
    "# Define paths\n",
    "path_GRACE = r\"C:\\Users\\YourUsername\\model\\GRACE TWSA\"\n",
    "path_model = r\"C:\\Users\\YourUsername\\model\\Results\\\\\" + model_name + \"\\\\Added Slope\"\n",
    "save_path = r\"C:\\Users\\YourUsername\\model\\Testing result\\\\\" + model_name + \"\\\\\"\n",
    "\n",
    "# Create save directory if not exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Get sorted list of files in GRACE folder\n",
    "files_list = [f for f in os.listdir(path_GRACE) if f.endswith('.tif')]\n",
    "files_list.sort(key=natural_keys)\n",
    "\n",
    "# Identify the relevant subset of files\n",
    "Data_to_find = ['2018-10.tif', files_list[-1]]\n",
    "indices = [index for index, value in enumerate(files_list) if value in Data_to_find]\n",
    "matrics_data = files_list[indices[0]:indices[1] + 1]\n",
    "\n",
    "# Read reference raster for metadata\n",
    "with rasterio.open(os.path.join(path_GRACE, files_list[0])) as ref:\n",
    "    ref_data = ref.read(1, masked=True)\n",
    "    profile = ref.profile\n",
    "\n",
    "# Initialize arrays for GRACE and model data\n",
    "raster_array_GRACE = []\n",
    "raster_array_model = []\n",
    "\n",
    "# Read data for the defined files\n",
    "for filename in matrics_data:\n",
    "    with rasterio.open(os.path.join(path_GRACE, filename)) as src_o:\n",
    "        GRACE_data = src_o.read(1, masked=True)\n",
    "        raster_array_GRACE.append(GRACE_data)\n",
    "\n",
    "    with rasterio.open(os.path.join(path_model, filename)) as src_p:\n",
    "        predicted_data = src_p.read(1, masked=True)\n",
    "        raster_array_model.append(predicted_data)\n",
    "\n",
    "# Convert data arrays to numpy\n",
    "n_sample, row, col = np.array(raster_array_GRACE).shape\n",
    "\n",
    "# Initialize metrics arrays\n",
    "RMSE_array = []\n",
    "NSE_array = []\n",
    "PCC_array = []\n",
    "\n",
    "# Compute metrics for each pixel\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        if np.array(raster_array_GRACE)[0, r, c] != -99999.0:\n",
    "            observed_temp = []\n",
    "            predicted_temp = []\n",
    "\n",
    "            # Collect observed and predicted values for all samples\n",
    "            for sample in range(n_sample):\n",
    "                observed_temp.append(np.array(raster_array_GRACE)[sample, r, c])\n",
    "                predicted_temp.append(np.array(raster_array_model)[sample, r, c])\n",
    "\n",
    "            # Calculate metrics for the current pixel\n",
    "            rmse, nse, PCC = calculate_matrix(np.array(observed_temp), np.array(predicted_temp))\n",
    "            RMSE_array.append(rmse)\n",
    "            NSE_array.append(nse)\n",
    "            PCC_array.append(PCC)\n",
    "        else:\n",
    "            # Assign NoData values for invalid pixels\n",
    "            RMSE_array.append(np.array(raster_array_GRACE)[0, r, c])\n",
    "            NSE_array.append(np.array(raster_array_GRACE)[0, r, c])\n",
    "            PCC_array.append(np.array(raster_array_GRACE)[0, r, c])\n",
    "\n",
    "# Reshape metric arrays to raster dimensions\n",
    "test_rmse = np.array(RMSE_array).reshape(row, col)\n",
    "test_nse = np.array(NSE_array).reshape(row, col)\n",
    "test_r = np.array(PCC_array).reshape(row, col)\n",
    "\n",
    "# Save metrics as raster files\n",
    "with rasterio.open(save_path + model_name + '_test_rmse.tif', \"w\", **profile) as dst:\n",
    "    dst.write(test_rmse, 1)\n",
    "\n",
    "with rasterio.open(save_path + model_name + '_test_nse.tif', \"w\", **profile) as dst:\n",
    "    dst.write(test_nse, 1)\n",
    "\n",
    "with rasterio.open(save_path + model_name + '_test_r.tif', \"w\", **profile) as dst:\n",
    "    dst.write(test_r, 1)\n",
    "\n",
    "print(\"File save Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec173291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
